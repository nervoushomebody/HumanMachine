{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwB0_JNo34ke",
        "outputId": "795761e4-629f-4235-930c-99ae12a1ec89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загальна кількість речень у корпусі: 3914\n",
            "Приклад речення: [('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]\n",
            "Кількість речень у навчальній вибірці: 3131\n",
            "Кількість речень у тестовій вибірці: 783\n",
            "Розмір словника (слова, що зустрічаються >= 2 рази): 5119\n",
            "Підготовка даних завершена. Файли збережено.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# Завантажуємо необхідні ресурси NLTK\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "# 1. Завантаження корпусу Treebank з універсальним набором тегів\n",
        "# Використання 'universal_tagset' спрощує кількість тегів (напр., 'NNP', 'NNS' стають 'NOUN')\n",
        "tagged_sents = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
        "\n",
        "print(f\"Загальна кількість речень у корпусі: {len(tagged_sents)}\")\n",
        "print(\"Приклад речення:\", tagged_sents[0])\n",
        "\n",
        "# 2. Перемішування та розділення на навчальну і тестову вибірки\n",
        "random.seed(42) # для відтворюваності результатів\n",
        "random.shuffle(tagged_sents)\n",
        "\n",
        "split_point = int(len(tagged_sents) * 0.8)\n",
        "train_sents = tagged_sents[:split_point]\n",
        "test_sents = tagged_sents[split_point:]\n",
        "\n",
        "print(f\"Кількість речень у навчальній вибірці: {len(train_sents)}\")\n",
        "print(f\"Кількість речень у тестовій вибірці: {len(test_sents)}\")\n",
        "\n",
        "# 3. Функція для запису даних у файли\n",
        "def write_tagged_sentences(sentences, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for sentence in sentences:\n",
        "            # Ігноруємо порожні речення, які можуть виникнути\n",
        "            if not sentence:\n",
        "                continue\n",
        "            for word, tag in sentence:\n",
        "                f.write(f\"{word}\\t{tag}\\n\")\n",
        "            f.write(\"\\n\") # Порожній рядок як роздільник між реченнями\n",
        "\n",
        "# Записуємо навчальну та тестову вибірки\n",
        "write_tagged_sentences(train_sents, \"treebank_training.pos\")\n",
        "write_tagged_sentences(test_sents, \"treebank_test.pos\")\n",
        "\n",
        "# 4. Створення та збереження словника\n",
        "word_counter = Counter(word for sentence in train_sents for word, tag in sentence)\n",
        "\n",
        "# Зберігаємо слова, які з'являються принаймні двічі\n",
        "vocab = {word for word, count in word_counter.items() if count >= 2}\n",
        "\n",
        "with open(\"treebank_vocab.txt\", 'w', encoding='utf-8') as f:\n",
        "    for word in sorted(list(vocab)):\n",
        "        f.write(f\"{word}\\n\")\n",
        "\n",
        "print(f\"Розмір словника (слова, що зустрічаються >= 2 рази): {len(vocab)}\")\n",
        "\n",
        "# 5. Створення файлу з тестовими словами для подальшого використання\n",
        "with open(\"treebank_test_words.txt\", 'w', encoding='utf-8') as f:\n",
        "    for sentence in test_sents:\n",
        "        if not sentence:\n",
        "            continue\n",
        "        for word, _ in sentence:\n",
        "            f.write(f\"{word}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(\"Підготовка даних завершена. Файли збережено.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def create_dictionaries(training_corpus_path, vocab):\n",
        "    \"\"\"\n",
        "    Створює словники для частот емісій, переходів та тегів.\n",
        "\n",
        "    Args:\n",
        "        training_corpus_path (str): Шлях до файлу з навчальним корпусом.\n",
        "        vocab (set): Словник відомих слів.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (emission_counts, transition_counts, tag_counts)\n",
        "    \"\"\"\n",
        "    emission_counts = defaultdict(int)\n",
        "    transition_counts = defaultdict(int)\n",
        "    tag_counts = defaultdict(int)\n",
        "\n",
        "    # Попередній тег ініціалізуємо як тег початку речення\n",
        "    prev_tag = '--s--'\n",
        "\n",
        "    with open(training_corpus_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "\n",
        "            # Якщо рядок порожній, це кінець речення\n",
        "            if not line:\n",
        "                # Перехід від останнього тега речення до стану \"початок наступного\"\n",
        "                transition_counts[(prev_tag, '--s--')] += 1\n",
        "                prev_tag = '--s--'\n",
        "                continue\n",
        "\n",
        "            word, tag = line.split('\\t')\n",
        "\n",
        "            # Обробка невідомих слів\n",
        "            if word not in vocab:\n",
        "                # В майбутньому ми замінимо це на більш складну логіку,\n",
        "                # а поки що використовуємо єдиний токен для невідомих слів.\n",
        "                word = '--unk--'\n",
        "\n",
        "            # Рахуємо переходи: (попередній_тег, поточний_тег)\n",
        "            transition_counts[(prev_tag, tag)] += 1\n",
        "\n",
        "            # Рахуємо емісії: (тег, слово)\n",
        "            emission_counts[(tag, word)] += 1\n",
        "\n",
        "            # Рахуємо загальну кількість кожного тега\n",
        "            tag_counts[tag] += 1\n",
        "\n",
        "            # Оновлюємо попередній тег\n",
        "            prev_tag = tag\n",
        "\n",
        "    return emission_counts, transition_counts, tag_counts\n",
        "\n",
        "# Створюємо словники на основі навчальних даних\n",
        "emission_counts, transition_counts, tag_counts = create_dictionaries(\"treebank_training.pos\", vocab)\n",
        "\n",
        "# Додаємо тег початку речення до лічильника тегів\n",
        "tag_counts['--s--'] = len(train_sents)\n",
        "\n",
        "print(\"Приклад частоти емісії:\", list(emission_counts.items())[0])\n",
        "print(\"Приклад частоти переходу:\", list(transition_counts.items())[0])\n",
        "print(\"Приклад частоти тега:\", list(tag_counts.items())[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZufwY0xQ42nd",
        "outputId": "d0d1600d-ad98-44ba-9434-125e7f572f48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Приклад частоти емісії: (('DET', 'The'), 584)\n",
            "Приклад частоти переходу: (('--s--', 'DET'), 736)\n",
            "Приклад частоти тега: ('DET', 7026)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
        "    \"\"\"\n",
        "    Створює матрицю переходів A з використанням згладжування.\n",
        "\n",
        "    Args:\n",
        "        alpha (float): Параметр згладжування.\n",
        "        tag_counts (dict): Словник з частотами тегів.\n",
        "        transition_counts (dict): Словник з частотами переходів.\n",
        "\n",
        "    Returns:\n",
        "        (np.array, dict, dict): Матриця A, словники для відображення тегів в індекси і навпаки.\n",
        "    \"\"\"\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    tag_to_idx = {tag: i for i, tag in enumerate(all_tags)}\n",
        "    idx_to_tag = {i: tag for i, tag in enumerate(all_tags)}\n",
        "\n",
        "    num_tags = len(all_tags)\n",
        "    A = np.zeros((num_tags, num_tags))\n",
        "\n",
        "    for i in range(num_tags):\n",
        "        prev_tag = idx_to_tag[i]\n",
        "\n",
        "        # Загальна кількість разів, коли зустрічався попередній тег\n",
        "        count_prev_tag = tag_counts[prev_tag]\n",
        "\n",
        "        for j in range(num_tags):\n",
        "            tag = idx_to_tag[j]\n",
        "\n",
        "            # Кількість переходів від prev_tag до tag\n",
        "            count_transition = transition_counts.get((prev_tag, tag), 0)\n",
        "\n",
        "            # Застосування згладжування\n",
        "            A[i, j] = (count_transition + alpha) / (count_prev_tag + alpha * num_tags)\n",
        "\n",
        "    return A, tag_to_idx, idx_to_tag\n",
        "\n",
        "# Створюємо матрицю переходів\n",
        "# alpha - це гіперпараметр, зазвичай обирають маленьке значення\n",
        "alpha = 0.001\n",
        "A, tag_to_idx, idx_to_tag = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
        "\n",
        "print(\"Розмір матриці переходів A:\", A.shape)\n",
        "# Ймовірність переходу від 'NOUN' до 'VERB'\n",
        "print(f\"P('VERB' | 'NOUN') = {A[tag_to_idx['NOUN'], tag_to_idx['VERB']]:.6f}\")\n",
        "# Ймовірність переходу від початку речення до іменника\n",
        "print(f\"P('NOUN' | '--s--') = {A[tag_to_idx['--s--'], tag_to_idx['NOUN']]:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeiyE3H547Z-",
        "outputId": "8d728ac3-3476-4a35-90e4-171d63650cf2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Розмір матриці переходів A: (13, 13)\n",
            "P('VERB' | 'NOUN') = 0.145418\n",
            "P('NOUN' | '--s--') = 0.290322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
        "    \"\"\"\n",
        "    Створює матрицю емісій B з використанням згладжування.\n",
        "\n",
        "    Args:\n",
        "        alpha (float): Параметр згладжування.\n",
        "        tag_counts (dict): Словник з частотами тегів.\n",
        "        emission_counts (dict): Словник з частотами емісій.\n",
        "        vocab (list): Список унікальних слів.\n",
        "\n",
        "    Returns:\n",
        "        (np.array, dict, dict): Матриця B, словники для відображення слів в індекси і навпаки.\n",
        "    \"\"\"\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    num_tags = len(all_tags)\n",
        "\n",
        "    # Додаємо токен для невідомих слів до словника\n",
        "    all_words = sorted(list(vocab)) + ['--unk--']\n",
        "    word_to_idx = {word: i for i, word in enumerate(all_words)}\n",
        "    idx_to_word = {i: word for i, word in enumerate(all_words)}\n",
        "\n",
        "    num_words = len(all_words)\n",
        "\n",
        "    B = np.zeros((num_tags, num_words))\n",
        "\n",
        "    for i in range(num_tags):\n",
        "        tag = all_tags[i]\n",
        "        count_tag = tag_counts[tag]\n",
        "\n",
        "        for j in range(num_words):\n",
        "            word = idx_to_word[j]\n",
        "            count_emission = emission_counts.get((tag, word), 0)\n",
        "\n",
        "            # Згладжування\n",
        "            B[i, j] = (count_emission + alpha) / (count_tag + alpha * num_words)\n",
        "\n",
        "    return B, word_to_idx, idx_to_word\n",
        "\n",
        "# Створюємо словник слів\n",
        "vocab_list = sorted(list(vocab))\n",
        "B, word_to_idx, idx_to_word = create_emission_matrix(alpha, tag_counts, emission_counts, vocab_list)\n",
        "\n",
        "\n",
        "print(\"Розмір матриці емісій B:\", B.shape)\n",
        "# Ймовірність, що тег 'NOUN' згенерує слово 'time'\n",
        "if 'time' in word_to_idx:\n",
        "    print(f\"P('time' | 'NOUN') = {B[tag_to_idx['NOUN'], word_to_idx['time']]:.6f}\")\n",
        "else:\n",
        "    print(\"'time' не є достатньо частим словом у навчальній вибірці.\")\n",
        "\n",
        "# Ймовірність, що тег 'VERB' згенерує слово 'is'\n",
        "if 'is' in word_to_idx:\n",
        "    print(f\"P('is' | 'VERB') = {B[tag_to_idx['VERB'], word_to_idx['is']]:.6f}\")\n",
        "else:\n",
        "    print(\"'is' не є достатньо частим словом у навчальній вибірці.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9bL7Mzl4_Pm",
        "outputId": "16e6eb2d-64fe-4c89-dca6-066ea5efabfa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Розмір матриці емісій B: (13, 5120)\n",
            "P('time' | 'NOUN') = 0.001982\n",
            "P('is' | 'VERB') = 0.050307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def viterbi(sentence, tag_to_idx, word_to_idx, A, B):\n",
        "    \"\"\"\n",
        "    Знаходить найкращу послідовність тегів для речення за допомогою алгоритму Вітербі.\n",
        "\n",
        "    Args:\n",
        "        sentence (list): Список слів у реченні.\n",
        "        tag_to_idx (dict): Словник для відображення тегів в індекси.\n",
        "        word_to_idx (dict): Словник для відображення слів в індекси.\n",
        "        A (np.array): Матриця переходів.\n",
        "        B (np.array): Матриця емісій.\n",
        "\n",
        "    Returns:\n",
        "        list: Найімовірніша послідовність тегів.\n",
        "    \"\"\"\n",
        "    num_tags = len(tag_to_idx)\n",
        "    T = len(sentence) # Довжина речення\n",
        "\n",
        "    # Ініціалізуємо матриці\n",
        "    viterbi_matrix = np.full((num_tags, T), -np.inf) # Матриця для зберігання ймовірностей\n",
        "    backpointer = np.zeros((num_tags, T), dtype=int) # Матриця для зберігання шляху\n",
        "\n",
        "    # 1. Ініціалізація (для першого слова)\n",
        "    start_tag_idx = tag_to_idx['--s--']\n",
        "    first_word_idx = word_to_idx.get(sentence[0], word_to_idx['--unk--'])\n",
        "\n",
        "    for tag_idx in range(num_tags):\n",
        "        # Перевіряємо, чи є перехід/емісія можливими (ймовірність > 0)\n",
        "        if A[start_tag_idx, tag_idx] > 0 and B[tag_idx, first_word_idx] > 0:\n",
        "            viterbi_matrix[tag_idx, 0] = math.log(A[start_tag_idx, tag_idx]) + math.log(B[tag_idx, first_word_idx])\n",
        "\n",
        "    # 2. Прямий хід (для решти слів)\n",
        "    for t in range(1, T):\n",
        "        word_idx = word_to_idx.get(sentence[t], word_to_idx['--unk--'])\n",
        "        for j in range(num_tags): # Поточний тег\n",
        "            max_prob = -np.inf\n",
        "            best_prev_tag_idx = -1\n",
        "\n",
        "            for i in range(num_tags): # Попередній тег\n",
        "                # Ймовірність переходу від попереднього тега до поточного\n",
        "                if A[i, j] > 0 and B[j, word_idx] > 0:\n",
        "                    prob = viterbi_matrix[i, t-1] + math.log(A[i, j]) + math.log(B[j, word_idx])\n",
        "                    if prob > max_prob:\n",
        "                        max_prob = prob\n",
        "                        best_prev_tag_idx = i\n",
        "\n",
        "            viterbi_matrix[j, t] = max_prob\n",
        "            backpointer[j, t] = best_prev_tag_idx\n",
        "\n",
        "    # 3. Зворотний хід\n",
        "    best_path = []\n",
        "\n",
        "    # Знаходимо останній тег\n",
        "    last_tag_idx = np.argmax(viterbi_matrix[:, T-1])\n",
        "    best_path.append(idx_to_tag[last_tag_idx])\n",
        "\n",
        "    # Йдемо назад по вказівниках\n",
        "    for t in range(T-1, 0, -1):\n",
        "        last_tag_idx = backpointer[last_tag_idx, t]\n",
        "        best_path.insert(0, idx_to_tag[last_tag_idx])\n",
        "\n",
        "    return best_path\n",
        "\n",
        "# Приклад використання\n",
        "test_sentence = [\"This\", \"is\", \"a\", \"test\", \".\"]\n",
        "predicted_tags = viterbi(test_sentence, tag_to_idx, word_to_idx, A, B)\n",
        "\n",
        "print(f\"Речення: {test_sentence}\")\n",
        "print(f\"Передбачені теги: {predicted_tags}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQt01F9j5Czc",
        "outputId": "7ee44fce-7f98-42b5-e838-b43c117aeb26"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Речення: ['This', 'is', 'a', 'test', '.']\n",
            "Передбачені теги: ['DET', 'VERB', 'DET', 'NOUN', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm # Для відображення прогресу\n",
        "\n",
        "def compute_accuracy(test_sents, tag_to_idx, word_to_idx, A, B):\n",
        "    \"\"\"\n",
        "    Обчислює точність моделі на тестових даних.\n",
        "    \"\"\"\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for sentence_tags in tqdm(test_sents, desc=\"Оцінка точності\"):\n",
        "        if not sentence_tags:\n",
        "            continue\n",
        "\n",
        "        sentence = [word for word, tag in sentence_tags]\n",
        "        gold_tags = [tag for word, tag in sentence_tags]\n",
        "\n",
        "        # Передбачаємо теги для речення\n",
        "        predicted_tags = viterbi(sentence, tag_to_idx, word_to_idx, A, B)\n",
        "\n",
        "        for pred_tag, gold_tag in zip(predicted_tags, gold_tags):\n",
        "            if pred_tag == gold_tag:\n",
        "                num_correct += 1\n",
        "            total += 1\n",
        "\n",
        "    return num_correct / total\n",
        "\n",
        "# Обчислюємо точність\n",
        "accuracy = compute_accuracy(test_sents, tag_to_idx, word_to_idx, A, B)\n",
        "print(f\"\\nТочність реалізованої моделі на тестовій вибірці: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3HJopwy5GA-",
        "outputId": "9aaa518b-722b-45f2-ad38-14f654bd75aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Оцінка точності: 100%|██████████| 783/783 [00:04<00:00, 158.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Точність реалізованої моделі на тестовій вибірці: 0.9297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger\n",
        "\n",
        "# 1. Визначаємо найчастіший тег для DefaultTagger\n",
        "most_common_tag = Counter(tag for sent in train_sents for word, tag in sent).most_common(1)[0][0]\n",
        "\n",
        "# 2. Створюємо та навчаємо тегери NLTK\n",
        "default_tagger = DefaultTagger(most_common_tag)\n",
        "unigram_tagger = UnigramTagger(train_sents, backoff=default_tagger)\n",
        "bigram_tagger = BigramTagger(train_sents, backoff=unigram_tagger)\n",
        "\n",
        "# 3. Оцінюємо точність тегера NLTK на тій самій тестовій вибірці\n",
        "nltk_accuracy = bigram_tagger.accuracy(test_sents)\n",
        "\n",
        "print(f\"Точність NLTK BigramTagger: {nltk_accuracy:.4f}\")\n",
        "print(f\"Точність нашої реалізації: {accuracy:.4f}\")\n",
        "print(f\"Різниця: {abs(accuracy - nltk_accuracy):.4f}\")\n",
        "\n",
        "# Висновок\n",
        "if accuracy > nltk_accuracy:\n",
        "    print(\"\\nНаша реалізація виявилася точнішою за стандартний BigramTagger з NLTK!\")\n",
        "else:\n",
        "    print(\"\\nСтандартний BigramTagger з NLTK виявився точнішим.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zno4YYc-5KmH",
        "outputId": "490a7a37-f6d6-4b0e-e990-e00370f3d9a3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точність NLTK BigramTagger: 0.9352\n",
            "Точність нашої реалізації: 0.9297\n",
            "Різниця: 0.0055\n",
            "\n",
            "Стандартний BigramTagger з NLTK виявився точнішим.\n"
          ]
        }
      ]
    }
  ]
}